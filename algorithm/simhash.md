# 转：基于局部敏感哈希的协同过滤算法之simHash算法
原文地址:[http://www.cnblogs.com/hxsyl/p/4456001.html](http://www.cnblogs.com/hxsyl/p/4456001.html)

搜集了快一个月的资料，虽然不完全懂，但还是先慢慢写着吧，说不定就有思路了呢。

开源的最大好处是会让作者对脏乱臭的代码有羞耻感。

当一个做推荐系统的部门开始重视【数据清理，数据标柱，效果评测，数据统计，数据分析】这些所谓的脏活累活，这样的推荐系统才会有救。

求教GitHub的使用。

简单不等于傻逼。

我为什么说累：我又是一个习惯在聊天中思考前因后果的人，所以整个大脑高负荷运转。不过这样真不好，学习学成傻逼了。

研一的最大收获是让我明白原来以前仰慕的各种国家自然基金项目，原来都是可以浑水摸鱼忽悠过去的，效率不高不说，还有可能有很多错误，哎，我就不说了。

# 一、问题来源

查找LSH发现的，这个是谷歌现在的网页去重方案。但是simHash和LSH有什么联系呢？提前透漏下，simHash本质上是一种LSH。正因为它的局部敏感性（这段的局部敏感性指的是非常相似的文本，即便只差一个字符，md5后的序列也可能非常不同，但是simHash之后的序列可能只是某几位不同），所以我们可以使用海明距离来衡量simhash值的相似度。

simhash是google用来处理海量文本去重的算法。google出品，你懂的。simhash最牛逼的一点就是将一个文档，最后转换成一个64位的字节，暂且称之为特征字，然后判断重复只需要判断他们的特征字的距离是不是<n（根据经验这个n一般取值为3），就可以判断两个文档是否相似。

谷歌出品嘛，简单实用。

# 二、算法解析

## 2.1 算法伪代码

1、分词，把需要判断文本分词形成这个文章的特征单词。最后形成去掉噪音词的单词序列并为每个词加上权重，我们假设权重分为5个级别（1~5）。比如：“ 美国“51区”雇员称内部有9架飞碟，曾看见灰色外星人 ” ==> 分词后为 “ 美国（4） 51区（5） 雇员（3） 称（1） 内部（2） 有（1） 9架（3） 飞碟（5） 曾（1） 看见（3） 灰色（4） 外星人（5）”，括号里是代表单词在整个句子里重要程度，数字越大越重要。

2、hash，通过hash算法把每个词变成hash值，比如“美国”通过hash算法计算为 100101,“51区”通过hash算法计算为 101011。这样我们的字符串就变成了一串串数字，还记得文章开头说过的吗，要把文章变为数字计算才能提高相似度计算性能，现在是降维过程进行时。

3、加权，通过 2步骤的hash生成结果，需要按照单词的权重形成加权数字串，比如“美国”的hash值为“100101”，通过加权计算为“4 -4 -4 4 -4 4”；“51区”的hash值为“101011”，通过加权计算为 “ 5 -5 5 -5 5 5”。

4、合并，把上面各个单词算出来的序列值累加，变成只有一个序列串。比如 “美国”的 “4 -4 -4 4 -4 4”，“51区”的 “ 5 -5 5 -5 5 5”， 把每一位进行累加， “4+5 -4+-5 -4+5 4+-5 -4+5 4+5” ==》 “9 -9 1 -1 1 9”。这里作为示例只算了两个单词的，真实计算需要把所有单词的序列串累加。

5、降维，把4步算出来的 “9 -9 1 -1 1 9” 变成 0 1 串，形成我们最终的simhash签名。 如果每一位大于0 记为 1，小于0 记为 0。最后算出结果为：“1 0 1 0 1 1”。

![](http://images.cnitblog.com/blog2015/387014/201504/251501233126555.jpg)

## 2.2 simHash和传统哈希的区别

大家可能会有疑问，经过这么多步骤搞这么麻烦，不就是为了得到个 0 1 字符串吗？我直接把这个文本作为字符串输入，用hash函数生成 0 1 值更简单。其实不是这样的，传统hash函数解决的是生成唯一值，比如 md5、hashmap等。md5是用于生成唯一签名串，只要稍微多加一个字符md5的两个数字看起来相差甚远；hashmap也是用于键值对查找，便于快速插入和查找的数据结构。不过我们主要解决的是文本相似度计算，要比较的是两个文章是否相识，当然我们降维生成了hashcode也是用于这个目的。看到这里估计大家就明白了，我们使用的simhash就算把文章中的字符串变成 01 串也还是可以用于计算相似度的，而传统的hashcode却不行。我们可以来做个测试，两个相差只有一个字符的文本串，“你妈妈喊你回家吃饭哦，回家罗回家罗” 和 “你妈妈叫你回家吃饭啦，回家罗回家罗”。

通过simhash计算结果为：

1000010010101101111111100000101011010001001111100001001011001011

1000010010101101011111100000101011010001001111100001101010001011

通过 hashcode计算为：

1111111111111111111111111111111110001000001100110100111011011110

1010010001111111110010110011101

大家可以看得出来，相似的文本只有部分 01 串变化了，而普通的hashcode却不能做到，这个就是局部敏感哈希的魅力。目前Broder提出的shingling算法和Charikar的simhash算法应该算是业界公认比较好的算法。

在simhash的发明人Charikar的论文中并没有给出具体的simhash算法和证明，“量子图灵”得出的证明simhash是由随机超平面hash算法演变而来的。

现在通过这样的转换，我们把库里的文本都转换为simhash 代码，并转换为long类型存储，空间大大减少。现在我们虽然解决了空间，但是如何计算两个simhash的相似度呢？难道是比较两simhash的01有多少个不同吗？对的，其实也就是这样，我们通过海明距离（Hamming distance）就可以计算出两个simhash到底相似不相似。两个simhash对应二进制（01串）取值不同的数量称为这两个simhash的海明距离。举例如下： 10101 和 00110 从第一位开始依次有第一位、第四、第五位不同，则海明距离为3。对于二进制字符串的a和b，海明距离为等于在a XOR b运算结果中1的个数（普遍算法）。

simhash和普通hash最大的不同在于传统的hash函数虽然也可以用于映射来比较文本的重复，但是对于可能差距只有一个字节的文档也会映射成两个完全不同的哈希结果，而simhash对相似的文本的哈希映射结果也相似。

http://www.lanceyan.com/tech/arch/simhash_hamming_distance_similarity.html

http://blog.sina.com.cn/s/blog_81e6c30b0101cpvu.html

权重该如何指派呢？我只知道用TF-IDF算法。

# 三、算法实现

谁有容易理解的java或者matlab代码，可以发短消息给我，一起为大家服务。 

# 四、对比其他去重算法

## 4.1 百度

百度的去重算法最简单，就是直接找出此文章的最长的n句话，做一遍hash签名。n一般取3。 工程实现巨简单，据说准确率和召回率都能到达80%以上。

百度的去重算法没有这么傻瓜吧？据一个百度凤巢出来的同事是这么说的。而且我个人觉得简单不等于傻逼。

## 4.2 shingle算法

shingle原理略复杂，不细说。 shingle算法我认为过于学院派，对于工程实现不够友好，速度太慢，基本上无法处理海量数据。

# 五、问题扩展

问题：一个80亿的64-bit指纹组成的集合Q，对于一个给定64-bit的指纹F，如何在a few millionseconds中找到Q中和f至多只有k(k=3)位差别的指纹。  
看文献吧。

我想的是能否借鉴AC自动机一类的算法，来做匹配，只不过匹配规则是海明距离小于3。我说的是优化后的精确匹配了。

http://grunt1223.iteye.com/blog/964564

http://blog.csdn.net/lgnlgn/article/details/6008498

http://blog.sina.com.cn/s/blog_81e6c30b0101cpvu.html